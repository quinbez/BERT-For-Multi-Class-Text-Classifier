{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinbez/BERT-For-Multi-Class-Text-Classifier/blob/main/BERT_for_Multi_class_text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT FOR TEXT CLASSIFICATION**"
      ],
      "metadata": {
        "id": "rvAnSPrW2qix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the libraries**"
      ],
      "metadata": {
        "id": "0gvy5T0h6Lct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "BI29E_BI6Pz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate for training\n",
        "lr = 1e-3\n",
        "\n",
        "# Sequence length (often used for sequences in NLP)\n",
        "seq_len = 20\n",
        "\n",
        "# Dropout rate for regularization\n",
        "dropout = 0.5\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 2\n",
        "\n",
        "# Column name for labels in the dataset\n",
        "label_col = \"Product\"\n",
        "\n",
        "# Path to the file containing tokenized data\n",
        "tokens_path = \"Output/tokens.pkl\"\n",
        "\n",
        "# Path to the file containing label data\n",
        "labels_path = \"Output/labels.pkl\"\n",
        "\n",
        "# Path to the input data file (complaints dataset)\n",
        "data_path = \"Input/complaints.csv\"\n",
        "\n",
        "# Path to save the pre-trained model\n",
        "model_path = \"Output/bert_pre_trained.pth\"\n",
        "\n",
        "# Column name for text data in the dataset\n",
        "text_col_name = \"Consumer complaint narrative\"\n",
        "\n",
        "# Path to the label encoder file\n",
        "label_encoder_path = \"Output/label_encoder.pkl\"\n",
        "\n",
        "# Mapping of product categories to their corresponding labels\n",
        "product_map = {\n",
        "    'Vehicle loan or lease': 'vehicle_loan',\n",
        "    'Credit reporting, credit repair services, or other personal consumer reports': 'credit_report',\n",
        "    'Credit card or prepaid card': 'card',\n",
        "    'Money transfer, virtual currency, or money service': 'money_transfer',\n",
        "    'virtual currency': 'money_transfer',\n",
        "    'Mortgage': 'mortgage',\n",
        "    'Payday loan, title loan, or personal loan': 'loan',\n",
        "    'Debt collection': 'debt_collection',\n",
        "    'Checking or savings account': 'savings_account',\n",
        "    'Credit card': 'card',\n",
        "    'Bank account or service': 'savings_account',\n",
        "    'Credit reporting': 'credit_report',\n",
        "    'Prepaid card': 'card',\n",
        "    'Payday loan': 'loan',\n",
        "    'Other financial service': 'others',\n",
        "    'Virtual currency': 'money_transfer',\n",
        "    'Student loan': 'loan',\n",
        "    'Consumer Loan': 'loan',\n",
        "    'Money transfers': 'money_transfer'\n",
        "}"
      ],
      "metadata": {
        "id": "c_0NndDD9nuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save and Load the File**"
      ],
      "metadata": {
        "id": "kw6DC60U-0-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def save_file(name, obj):\n",
        "    \"\"\"\n",
        "    Function to save an object as a pickle file.\n",
        "\n",
        "    :param name: The name of the pickle file to save.\n",
        "    :param obj: The object to be saved.\n",
        "    \"\"\"\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def load_file(name):\n",
        "    \"\"\"\n",
        "    Function to load a pickle object.\n",
        "\n",
        "    :param name: The name of the pickle file to load.\n",
        "    :return: The loaded object from the pickle file.\n",
        "    \"\"\"\n",
        "    return pickle.load(open(name, \"rb\"))"
      ],
      "metadata": {
        "id": "8stKmrP8-uza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing the data**"
      ],
      "metadata": {
        "id": "5Bok3hbW6ljB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "AXMQwSwj6uxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset=[text_col_name], inplace=True)"
      ],
      "metadata": {
        "id": "M1jwK8H76wwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.replace({label_col: product_map}, inplace=True)"
      ],
      "metadata": {
        "id": "GOT-35ag6yGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encode labels**"
      ],
      "metadata": {
        "id": "AeBFcynH6zHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the LabelEncoder to the labels in the 'label_col' column of your data\n",
        "label_encoder.fit(data[label_col])\n",
        "\n",
        "# Transform (encode) the labels in the 'label_col' column into numerical values\n",
        "labels = label_encoder.transform(data[label_col])"
      ],
      "metadata": {
        "id": "EaTTTyKT63h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "save_file(labels_path, labels)\n",
        "save_file(label_encoder_path, label_encoder)"
      ],
      "metadata": {
        "id": "30MBGKNd65Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process the text column**"
      ],
      "metadata": {
        "id": "iwwRniX266OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data from 'data' into a list\n",
        "text_list = list(data[text_col_name])"
      ],
      "metadata": {
        "id": "FaUA2SCF6-yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_text)"
      ],
      "metadata": {
        "id": "dc5tQpz-7A-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert text to lower case**"
      ],
      "metadata": {
        "id": "eWyym3dH7CHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase with progress bar\n",
        "input_text = [i.lower() for i in tqdm(input_text)]"
      ],
      "metadata": {
        "id": "XfhCJmhf7HBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove punctuations except apostrophe**"
      ],
      "metadata": {
        "id": "HeT8jX4q7ImW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace non-alphanumeric characters with spaces in text with progress bar\n",
        "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i) for i in tqdm(input_text)]"
      ],
      "metadata": {
        "id": "AkdqXLDX7MON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove digits**"
      ],
      "metadata": {
        "id": "Lh2eokIU7TXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove digits from text with progress bar\n",
        "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"
      ],
      "metadata": {
        "id": "Do_iSb4E7ask"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove more than one consecutive instance of 'x'**"
      ],
      "metadata": {
        "id": "_yBEcYon7cnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove consecutive 'x' characters (2 or more) from text with progress bar\n",
        "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"
      ],
      "metadata": {
        "id": "gWUELl9k7mO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove multiple spaces with single space**"
      ],
      "metadata": {
        "id": "DukhtijI7tDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace multiple spaces with a single space in text with progress bar\n",
        "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"
      ],
      "metadata": {
        "id": "mm7df1jV7wgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize the text**"
      ],
      "metadata": {
        "id": "YDvNoW7k7ySh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer using the 'bert-base-cased' model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "tG7qjomC71xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text[0]"
      ],
      "metadata": {
        "id": "6qaKeO3J720_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the first text sample using the tokenizer\n",
        "sample_tokens = tokenizer(input_text[0], padding=\"max_length\", max_length=seq_len, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "yrNTdArZ76R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens"
      ],
      "metadata": {
        "id": "-yqbGhIt7-r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens[\"input_ids\"]"
      ],
      "metadata": {
        "id": "fIfu5nXS7__5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "qASxUUf88BPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize each text in the input_text list with specified settings and create a list of tokens\n",
        "\n",
        "tokens = [tokenizer(i, padding=\"max_length\", max_length=seq_len, truncation=True, return_tensors=\"pt\") for i in tqdm(input_text)]\n"
      ],
      "metadata": {
        "id": "Htf62DQe8DQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save the tokens**"
      ],
      "metadata": {
        "id": "ukJ2ks3b8HNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(tokens_path, tokens)"
      ],
      "metadata": {
        "id": "uAqZn8xg8LH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset**"
      ],
      "metadata": {
        "id": "FbLdb_iL5n3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokens, labels):\n",
        "        \"\"\"\n",
        "        Initialize a TextDataset.\n",
        "\n",
        "        Args:\n",
        "            tokens (list): List of tokenized text data.\n",
        "            labels (list): List of corresponding labels.\n",
        "\n",
        "        This class is designed to work as a PyTorch Dataset, which means it can be used with PyTorch's DataLoader for efficient data loading during training and evaluation.\n",
        "        \"\"\"\n",
        "        self.tokens = tokens  # List of tokenized text data\n",
        "        self.labels = labels  # List of corresponding labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the total number of data points in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of data points in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a specific data point (a pair of text data and its label) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the data point to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the label and tokenized text data for the specified data point.\n",
        "        \"\"\"\n",
        "        return self.labels[idx], self.tokens[idx]"
      ],
      "metadata": {
        "id": "kWn_TtW15tAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT CLASSIFIER MODEL**"
      ],
      "metadata": {
        "id": "o4pD8ndM46t8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzeI1cuYtQgd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False  # Freeze BERT parameters\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, num_classes)  # 768 is the BERT hidden size\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Perform a forward pass through the model\n",
        "        _, bert_output = self.bert(input_ids=input_ids,\n",
        "                                   attention_mask=attention_mask,\n",
        "                                   return_dict=False)\n",
        "        dropout_output = self.activation(self.dropout(bert_output))\n",
        "        final_output = self.linear(dropout_output)\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training The Model**"
      ],
      "metadata": {
        "id": "XO6bBn7e5CTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "          device, num_epochs, model_path):\n",
        "    \"\"\"\n",
        "    Function to train the model\n",
        "    :param train_loader: Data loader for the train dataset\n",
        "    :param valid_loader: Data loader for the validation dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param optimizer: Optimizer\n",
        "    :param device: CUDA or CPU\n",
        "    :param num_epochs: Number of epochs\n",
        "    :param model_path: Path to save the model\n",
        "    \"\"\"\n",
        "    best_loss = 1e8\n",
        "    for i in range(num_epochs):\n",
        "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
        "        valid_loss, train_loss = [], []\n",
        "        model.train()  # Set the model in training mode\n",
        "        # Train loop\n",
        "        for batch_labels, batch_data in tqdm(train_loader):\n",
        "            input_ids = batch_data[\"input_ids\"]\n",
        "            attention_mask = batch_data[\"attention_mask\"]\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            input_ids = torch.squeeze(input_ids, 1)\n",
        "            # Forward pass\n",
        "            batch_output = model(input_ids, attention_mask)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            train_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Gradient update step\n",
        "            optimizer.step()\n",
        "        model.eval()  # Set the model in evaluation mode\n",
        "        # Validation loop\n",
        "        for batch_labels, batch_data in tqdm(valid_loader):\n",
        "            input_ids = batch_data[\"input_ids\"]\n",
        "            attention_mask = batch_data[\"attention_mask\"]\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            input_ids = torch.squeeze(input_ids, 1)\n",
        "            # Forward pass\n",
        "            batch_output = model(input_ids, attention_mask)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            valid_loss.append(loss.item())\n",
        "        t_loss = np.mean(train_loss)\n",
        "        v_loss = np.mean(valid_loss)\n",
        "        print(f\"Train Loss: {t_loss}, Validation Loss: {v_loss}\")\n",
        "        if v_loss < best_loss:\n",
        "            best_loss = v_loss\n",
        "            # Save the model if validation loss improves\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Best Validation Loss: {best_loss}\")"
      ],
      "metadata": {
        "id": "mHt_vV9X5IN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing the Model**"
      ],
      "metadata": {
        "id": "HP9-rxxZ5Ki7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Function to test the model\n",
        "    :param test_loader: Data loader for the test dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param device: CUDA or CPU\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "    test_loss = []\n",
        "    test_accu = []\n",
        "    for batch_labels, batch_data in tqdm(test_loader):\n",
        "        input_ids = batch_data[\"input_ids\"]\n",
        "        attention_mask = batch_data[\"attention_mask\"]\n",
        "        # Move data to GPU if available\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        input_ids = torch.squeeze(input_ids, 1)\n",
        "        # Forward pass\n",
        "        batch_output = model(input_ids, attention_mask)\n",
        "        batch_output = torch.squeeze(batch_output)\n",
        "        # Calculate loss\n",
        "        loss = criterion(batch_output, batch_labels)\n",
        "        test_loss.append(loss.item())\n",
        "        batch_preds = torch.argmax(batch_output, axis=1)\n",
        "        # Move predictions to CPU\n",
        "        if torch.cuda.is_available():\n",
        "            batch_labels = batch_labels.cpu()\n",
        "            batch_preds = batch_preds.cpu()\n",
        "        # Compute accuracy\n",
        "        test_accu.append(accuracy_score(batch_labels.detach().numpy(),\n",
        "                                        batch_preds.detach().numpy()))\n",
        "    test_loss = np.mean(test_loss)\n",
        "    test_accu = np.mean(test_accu)\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc})"
      ],
      "metadata": {
        "id": "0ZQKSxwD43sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save and Load the File**"
      ],
      "metadata": {
        "id": "NrPszOWj4pWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the files**"
      ],
      "metadata": {
        "id": "zatUmiiq8i68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load token data, labels, and label encoder\n",
        "tokens = load_file(tokens_path)\n",
        "labels = load_file(labels_path)\n",
        "label_encoder = load_file(label_encoder_path)\n",
        "\n",
        "# Get the number of classes from the label encoder\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "4CZi6ddK8lWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split data into train, validation and test sets**"
      ],
      "metadata": {
        "id": "tr3XMWAa8nRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(tokens, labels, test_size=0.2)\n",
        "\n",
        "# Further split the training data into training and validation sets (60% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25)"
      ],
      "metadata": {
        "id": "8SaaerWb8sSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create PyTorch datasets**"
      ],
      "metadata": {
        "id": "6PcOR34F8u7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training, validation, and test datasets\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "valid_dataset = TextDataset(X_valid, y_valid)\n",
        "test_dataset = TextDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "HRMaPpt08yKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create data loaders**"
      ],
      "metadata": {
        "id": "jcfOBJTJ80Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for training, validation, and test datasets\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "52ErhvMG82UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create model object**"
      ],
      "metadata": {
        "id": "Nb0V9Orp841y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
        "                     else \"cpu\")"
      ],
      "metadata": {
        "id": "2QaiCnhx86Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BERT-based classifier model\n",
        "model = BertClassifier(dropout, num_classes)"
      ],
      "metadata": {
        "id": "wx8KAROI88_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define loss function and optimizer**"
      ],
      "metadata": {
        "id": "dA87W5mf8-TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Define CrossEntropyLoss for the criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize an Adam optimizer for model parameters with a specified learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "8yszy5gY8_fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Move the model to GPU if available**"
      ],
      "metadata": {
        "id": "9LcuOeIl9B88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "Zq4S5UVR9DSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training loop**\n"
      ],
      "metadata": {
        "id": "AXkPX7NS9Fdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train(train_loader, valid_loader, model, criterion, optimizer, device, num_epochs, model_path)"
      ],
      "metadata": {
        "id": "bwczWR969Ge8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the model**"
      ],
      "metadata": {
        "id": "020i3P6w9JV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_loader, model, criterion, device)"
      ],
      "metadata": {
        "id": "0fle4gUX9KbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict on new text**"
      ],
      "metadata": {
        "id": "uZ4RYgsK9NKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_text = '''I am a victim of Identity Theft & currently have an Experian account that\n",
        "I can view my Experian Credit Report and getting notified when there is activity on\n",
        "my Experian Credit Report. For the past 3 days I've spent a total of approximately 9\n",
        "hours on the phone with Experian. Every time I call I get transferred repeatedly and\n",
        "then my last transfer and automated message states to press 1 and leave a message and\n",
        "someone would call me. Every time I press 1 I get an automatic message stating than you\n",
        "before I even leave a message and get disconnected. I call Experian again, explain what\n",
        "is happening and the process begins again with the same end result. I was trying to have\n",
        "this issue attended and resolved informally but I give up after 9 hours. There are hard\n",
        "hit inquiries on my Experian Credit Report that are fraud, I didn't authorize, or recall\n",
        "and I respectfully request that Experian remove the hard hit inquiries immediately just\n",
        "like they've done in the past when I was able to speak to a live Experian representative\n",
        "in the United States. The following are the hard hit inquiries : BK OF XXXX XX/XX/XXXX\n",
        "XXXX XXXX XXXX  XX/XX/XXXX XXXX  XXXX XXXX  XX/XX/XXXX XXXX  XX/XX/XXXX XXXX  XXXX\n",
        "XX/XX/XXXX'''"
      ],
      "metadata": {
        "id": "AdJyRFae9Oc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "input_text = input_text.lower()\n",
        "\n",
        "# Replace non-alphanumeric characters with spaces\n",
        "input_text = re.sub(r\"[^\\w\\d'\\s]+\", \" \", input_text)\n",
        "\n",
        "# Remove digits from text\n",
        "input_text = re.sub(\"\\d+\", \"\", input_text)\n",
        "\n",
        "# Remove consecutive 'x' characters (2 or more)\n",
        "input_text = re.sub(r'[x]{2,}', \"\", input_text)\n",
        "\n",
        "# Replace multiple spaces with a single space\n",
        "input_text = re.sub(' +', ' ', input_text)"
      ],
      "metadata": {
        "id": "4azQ8DkG9RMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer using the 'bert-base-cased' model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "3dIjL6g59Snv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input_text with specified settings\n",
        "tokens = tokenizer(input_text, padding=\"max_length\", max_length=seq_len, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "DaBoTLU69UG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input IDs from the tokenized data\n",
        "input_ids = tokens[\"input_ids\"]\n",
        "\n",
        "# Get attention mask from the tokenized data\n",
        "attention_mask = tokens[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "qTVQyj6s9VY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
        "                     else \"cpu\")"
      ],
      "metadata": {
        "id": "Ux_0QIHe9WxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move input IDs to the specified device (GPU or CPU)\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Move attention mask to the specified device (GPU or CPU)\n",
        "attention_mask = attention_mask.to(device)"
      ],
      "metadata": {
        "id": "sebHSnzl9YHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Squeeze the input_ids tensor along the specified dimension (1)\n",
        "input_ids = torch.squeeze(input_ids, 1)"
      ],
      "metadata": {
        "id": "FqUQiqK89ZFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the label encoder from a file\n",
        "label_encoder = load_file(label_encoder_path)\n",
        "\n",
        "# Get the number of classes from the label encoder\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "jmUdXgfj9aRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model object\n",
        "model = BertClassifier(dropout, num_classes)\n",
        "\n",
        "# Load trained weights into the model\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Forward pass and get model output\n",
        "out = torch.squeeze(model(input_ids, attention_mask))\n",
        "\n",
        "# Find the predicted class using the label encoder\n",
        "prediction = label_encoder.classes_[torch.argmax(out)]\n",
        "print(f\"Predicted Class: {prediction}\")"
      ],
      "metadata": {
        "id": "GuJngGCc9bjl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}